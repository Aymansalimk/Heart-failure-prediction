{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLqP2qpPyCciiQHEM6CFPr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wY-p7R5tdVal","executionInfo":{"status":"ok","timestamp":1710579338782,"user_tz":-330,"elapsed":18135,"user":{"displayName":"AYMAN K (RA2011026020003)","userId":"02338098250338351978"}},"outputId":"91a58e62-21eb-4cc7-8125-a67d2adf9c99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Metrics:\n","Best Parameters: {'max_depth': 5, 'min_samples_split': 2}\n","Accuracy: 0.81\n","Precision: 0.72\n","Recall: 0.63\n","F1 Score: 0.67\n","Random Forest Metrics:\n","Best Parameters: {'max_depth': 20, 'n_estimators': 100}\n","Accuracy: 0.88\n","Precision: 0.85\n","Recall: 0.79\n","F1 Score: 0.82\n","SVM Metrics:\n","Best Parameters: {'C': 1, 'kernel': 'linear'}\n","Accuracy: 0.82\n","Precision: 0.72\n","Recall: 0.65\n","F1 Score: 0.68\n","Logistic Regression Metrics:\n","Best Parameters: {'C': 1, 'penalty': 'l2'}\n","Accuracy: 0.83\n","Precision: 0.76\n","Recall: 0.62\n","F1 Score: 0.67\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n","15 fits failed out of a total of 30.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","15 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.81152482        nan 0.8322695         nan 0.83218085]\n","  warnings.warn(\n"]}],"source":["\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n","\n","# Load the dataset (modify the path to your local file)\n","data = pd.read_csv(\"heart_failure_clinical_records_dataset.csv\")\n","\n","# Define feature columns and target variable\n","X = data.drop(columns=\"DEATH_EVENT\")\n","y = data[\"DEATH_EVENT\"]\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Create and train machine learning models with hyperparameter tuning\n","\n","models = {\n","    \"Decision Tree\": DecisionTreeClassifier(),\n","    \"Random Forest\": RandomForestClassifier(),\n","    \"SVM\": SVC(),\n","    \"Logistic Regression\": LogisticRegression()\n","}\n","\n","# Hyperparameter tuning using GridSearchCV\n","param_grid = {\n","    \"Decision Tree\": {\"max_depth\": [None, 5, 10, 20], \"min_samples_split\": [2, 5, 10]},\n","    \"Random Forest\": {\"n_estimators\": [100, 200, 300], \"max_depth\": [None, 10, 20]},\n","    \"SVM\": {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]},\n","    \"Logistic Regression\": {\"C\": [0.1, 1, 10], \"penalty\": [\"l1\", \"l2\"]}\n","}\n","\n","results = {}\n","for model_name, model in models.items():\n","    grid_search = GridSearchCV(model, param_grid[model_name], cv=5, scoring='accuracy', n_jobs=-1)\n","    grid_search.fit(X_train, y_train)\n","    best_model = grid_search.best_estimator_\n","\n","    # Evaluate the best model with cross-validation\n","    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n","    accuracy = cv_scores.mean()\n","    precision = cross_val_score(best_model, X_train, y_train, cv=5, scoring=make_scorer(precision_score))\n","    recall = cross_val_score(best_model, X_train, y_train, cv=5, scoring=make_scorer(recall_score))\n","    f1 = cross_val_score(best_model, X_train, y_train, cv=5, scoring=make_scorer(f1_score))\n","\n","    results[model_name] = {\n","        \"Best Parameters\": grid_search.best_params_,\n","        \"Accuracy\": accuracy,\n","        \"Precision\": precision.mean(),\n","        \"Recall\": recall.mean(),\n","        \"F1 Score\": f1.mean()\n","    }\n","\n","# Print the model performance metrics after hyperparameter tuning and cross-validation\n","for model_name, metrics in results.items():\n","    print(f\"{model_name} Metrics:\")\n","    for metric_name, value in metrics.items():\n","        if isinstance(value, dict):  # Handling dictionaries within the metrics\n","            print(f\"{metric_name}: {value}\")\n","        else:\n","            print(f\"{metric_name}: {value:.2f}\")"]}]}